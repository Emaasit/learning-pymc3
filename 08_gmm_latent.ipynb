{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gaussian Mixture Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import theano\n",
    "import theano.tensor as tt\n",
    "import pymc3 as pm\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "sns.set_context('notebook')\n",
    "np.random.seed(12345)\n",
    "rc = {'xtick.labelsize': 10, 'ytick.labelsize': 10, 'axes.labelsize': 10, 'font.size': 10, \n",
    "      'legend.fontsize': 12.0, 'axes.titlesize': 10, \"figure.figsize\": [14, 6]}\n",
    "sns.set(rc = rc)\n",
    "sns.set_style(\"whitegrid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Prepare the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simulate fake data with known mixture distribution\n",
    "np.random.seed(123456)\n",
    "\n",
    "K = 3\n",
    "N = 1000\n",
    "pi = np.array([0.35, 0.4, 0.25])\n",
    "means = np.array([0, 5, 10])\n",
    "sigmas = np.array([0.5, 0.5, 1.0])\n",
    "\n",
    "components = np.random.randint(0, K, N) # also called latent z\n",
    "fake_data = np.random.normal(loc = means[components], scale = sigmas[components])\n",
    "y = theano.shared(fake_data)\n",
    "#y.get_value()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(fake_data, bins = 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Build the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alternative 1: As a latent variable model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A natural parameterization of the Gaussian mixture model is as the latent variable model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{split}\\begin{align*}\n",
    "\\mu_1, \\ldots, \\mu_K\n",
    "    & \\sim N(0, \\sigma^2) \\\\\n",
    "\\tau_1, \\ldots, \\tau_K\n",
    "    & \\sim \\textrm{Gamma}(a, b) \\\\\n",
    "\\boldsymbol{\\pi}\n",
    "    & \\sim \\textrm{Dir}(\\boldsymbol{\\alpha}) \\\\\n",
    "z\\ |\\ \\boldsymbol{\\pi}\n",
    "    & \\sim \\textrm{Cat}(\\boldsymbol{\\pi}) \\\\\n",
    "x\\ |\\ z\n",
    "    & \\sim N(\\mu_z, \\tau^{-1}_z).\n",
    "\\end{align*}\\end{split}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pm.Model() as gmm_latent:\n",
    "    \n",
    "    # specify the priors\n",
    "    pi = pm.Dirichlet(\"pi\", a = np.array([1.0, 1.0, 1.0]), shape = K)\n",
    "    means = pm.Normal(\"means\", mu = [0, 0, 0], sd = 15, shape = K)\n",
    "    std = pm.Gamma(\"std\", alpha = 0.5, beta = 1, shape = K)\n",
    "    \n",
    "    # ensure all clusters have some points\n",
    "    p_min_potential = pm.Potential('p_min_potential', tt.switch(tt.min(pi) < .1, -np.inf, 0))\n",
    "    \n",
    "    # break symmetry\n",
    "    order_means_potential = pm.Potential('order_means_potential',\n",
    "                                         tt.switch(means[1]-means[0] < 0, -np.inf, 0)\n",
    "                                         + tt.switch(means[2]-means[1] < 0, -np.inf, 0))\n",
    "\n",
    "    # latent cluster for each observation\n",
    "    category = pm.Categorical(\"category\", p = pi, shape = N)\n",
    "    \n",
    "    # likelihood for each observation\n",
    "    obs = pm.Normal(\"obs\", mu = means[category], sd = std[category], observed = y)\n",
    "    \n",
    "    # simulate data from this model\n",
    "    simulated_obs = pm.Normal(\"simulated_obs\", mu = means[category], sd = std[category], shape = means[category].tag.test_value.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alternative 2: Marginalize out the latent variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A drawback of alternative 1 is that the posterior relies on sampling the discrete latent variable $z_z$. This reliance can cause slow mixing and ineffective exploration of the tails of the distribution. An alternative, equivalent parameterization that addresses these problems is to marginalize over $z_z$. The marginalized model is"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{split}\\begin{align*}\n",
    "\\mu_1, \\ldots, \\mu_K\n",
    "    & \\sim N(0, \\sigma^2) \\\\\n",
    "\\tau_1, \\ldots, \\tau_K\n",
    "    & \\sim \\textrm{Gamma}(a, b) \\\\\n",
    "\\boldsymbol{\\pi}\n",
    "    & \\sim \\textrm{Dir}(\\boldsymbol{\\alpha}) \\\\\n",
    "f(y\\ |\\ \\boldsymbol{\\pi})\n",
    "    & = \\sum_{i = 1}^K w_i\\ N(y\\ |\\ \\mu_i, \\tau^{-1}_z),\n",
    "\\end{align*}\\end{split}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pm.Model() as gmm_marginalized:\n",
    "    \n",
    "    # specify the priors\n",
    "    pi = pm.Dirichlet(\"pi\", a = np.array([1.0, 1.0, 1.0]), shape = K)\n",
    "    means = pm.Normal(\"means\", mu = [0, 0, 0], sd = 15, shape = K)\n",
    "    std = pm.Uniform(\"std\", lower = 0, upper = 20)\n",
    "    \n",
    "    # specify the likelihood\n",
    "    obs = pm.NormalMixture(\"obs\", w = pi, mu = means, observed = y) \n",
    "    \n",
    "    # simulate data from this model\n",
    "    simulated_obs = pm.NormalMixture(\"simulated_obs\", w = pi, mu = means, sd = std, shape = means.tag.test_value.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Sample from the posterior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with gmm_latent:\n",
    "    posterior_latent = pm.sample(draws = 5000, n_init = 10000, njobs = 2, tune = 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Diagnose convergence of MCMC chains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pm.traceplot(posterior_latent, varnames = [\"pi\", \"means\", \"std\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pm.gelman_rubin(posterior_latent, varnames = [\"pi\", \"means\", \"std\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pm.energyplot(posterior)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pm.forestplot(posterior_latent, varnames = [\"pi\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pm.forestplot(posterior_latent, varnames = [\"means\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pm.forestplot(posterior_latent, varnames = [\"std\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pm.autocorrplot(posterior_latent, varnames = [\"pi\", \"means\", \"std\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Criticize the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pm.plot_posterior(posterior_latent, varnames = [\"pi\", \"means\", \"std\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pm.summary(posterior_latent, varnames = [\"pi\", \"means\", \"std\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simulated_data = posterior_latent[\"simulated_obs\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simulated_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(simulated_data.mean(axis = 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6: Use the model for prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K = 3\n",
    "N = 50\n",
    "pi = np.array([0.35, 0.4, 0.25])\n",
    "means = np.array([0, 5, 10])\n",
    "sigmas = np.array([0.5, 0.5, 1.0])\n",
    "\n",
    "components = np.random.randint(0, K, N) # also called latent z\n",
    "new_obs = np.random.normal(loc = means[components], scale = sigmas[components])\n",
    "y.set_value(new_obs)\n",
    "#y.get_value()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_obs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use pm.sample_ppc to do posterior predictive checks\n",
    "with gmm_marginalized:\n",
    "    post_pred_latent = pm.sample_ppc(posterior_latent, samples = 3000, size = len(new_obs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "post_pred_marginalized.values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax = sns.distplot(post_pred_latent['obs'].mean(axis = 1), label = 'Posterior predictive distribution')\n",
    "ax.axvline(means[0], color='r', ls='--', label='True mean of $Z_1$')\n",
    "ax.axvline(means[1], color='b', ls='--', label='True mean of $Z_2$')\n",
    "ax.axvline(means[2], color='g', ls='--', label='True mean of $Z_3$')\n",
    "ax.legend()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
